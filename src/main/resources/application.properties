# Application Configuration
spring.application.name=BasicAiResponder

# Server Configuration
# WHY port 8080? Standard Spring Boot default, familiar to developers
server.port=8080

# OpenAI API Configuration
# WHY environment variable for API key?
# 1. Security: Keeps secrets out of source code
# 2. Environment-specific: Different keys for dev/test/prod
# 3. Best practice: Follows 12-factor app principles
# 4. Flexibility: Easy to change without redeployment
spring.ai.openai.api-key=${OPENAI_API_KEY}

# Model Configuration
# WHY gpt-3.5-turbo for branch 1?
# 1. Cost-effective: Much cheaper than GPT-4 for learning
# 2. Fast responses: Better for interactive development
# 3. Sufficient capability: Handles basic Q&A and demonstrations
# 4. Wide availability: Most users have access to this model
spring.ai.openai.chat.options.model=gpt-3.5-turbo

# WHY temperature=0.7?
# 1. Balanced creativity: Not too random (1.0) or too deterministic (0.0)
# 2. Consistent for learning: Responses are reasonably predictable
# 3. Good for demos: Interesting but not wildly unpredictable responses
# 4. Industry standard: Common setting for general-purpose applications
spring.ai.openai.chat.options.temperature=0.7

# WHY max-tokens=150?
# 1. Cost control: Limits expense during development and testing
# 2. Focused responses: Encourages concise, relevant answers
# 3. Performance: Faster response times with shorter outputs
# 4. Learning: Students see complete responses without overwhelming detail
# Note: Will be increased in later branches as complexity grows
spring.ai.openai.chat.options.maxTokens=150